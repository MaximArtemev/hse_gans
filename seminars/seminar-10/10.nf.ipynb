{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/MaximArtemev/hse_gans/blob/master/seminars/seminar-10/10.nf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is taken from [rep](https://github.com/tonyduan/normalizing-flows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from comet_ml import Experiment\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import itertools\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from argparse import ArgumentParser\n",
    "from torch.distributions import MultivariateNormal\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from nf.flows import *\n",
    "from nf.models import NormalizingFlowModel\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "**Problem:** how to estimate the pdf of final distribution at each point? \n",
    "\n",
    "**Idea:** Let's define the bijection $z_k=f(z_0)$ between simple distribution of $z_0$ with known pdf and our distribution $z_k$ with unknown pdf\n",
    "![](https://2.bp.blogspot.com/-g37e2x1miRo/Wl-g8ajU11I/AAAAAAAAHkY/PbIorxOav_Y61yFJeXsQLRlcKTzlkykYwCLcBGAs/s1600/shakir_danilo_slide.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem:** ... but known pdf is changed at each point after each transformation $f$\n",
    "![](https://2.bp.blogspot.com/-1vyL7LpM1io/Wl-ghB0yOiI/AAAAAAAAHkM/_U94kuVeQpk22J5Mg0lbLK-EdMDkaQWggCLcBGAs/s1600/flow1.png)\n",
    "\n",
    "**Solution:** The Jacobian is exactly the factor how volume is changed at each point $$J_k=|\\frac{\\partial f_k}{\\partial z_k}|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can stack multiple transformations f\n",
    "![](https://lilianweng.github.io/lil-log/assets/images/normalizing-flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.google.ru/url?sa=i&rct=j&q=&esrc=s&source=images&cd=&cad=rja&uact=8&ved=2ahUKEwjHu6ivj4rmAhWQAxAIHUxYDcIQjRx6BAgBEAQ&url=https%3A%2F%2Fblog.evjang.com%2F2018%2F01%2Fnf1.html&psig=AOvVaw3yZP21Fikrtn_pqZaUAr21&ust=1574934641464249)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the final pdf of our distribution can be evaluated as \n",
    "\n",
    "$$p(z_k)=\\frac{p(z_0)}{\\Pi_{i=1}^k J_i}$$\n",
    "\n",
    "or, \n",
    "\n",
    "$$log(p(z_k))=log(p(z_0))-\\Sigma_{i=1}^klog(J_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of transformations\n",
    "- Planar flows; $f(x) = x + u h(w^\\intercal z + b)$\n",
    "- Radial flows; $f(x) = x + \\frac{\\beta}{\\alpha + |x - x_0|}(x - x_0)$\n",
    "- Real NVP; affine coupling layer; $f(x^{(2)}) = t(x^{(1)}) + x^{(2)}\\odot\\exp s(x^{(1)}) $\n",
    "- Masked Autoregressive Flow (MAF); $f(x_i) = (x_i - \\mu(x_{<i})) / \\exp(\\alpha(x_{<i}))$\n",
    "- Invertible 1x1 Convolution (Glow);\n",
    "- ActNorm; $f(x) = Wx + b$ where $W$ is diagonal and $b$ is a constant\n",
    "- Autoregressive Neural Spline Flow (NSF-AF); $f(x_i) = \\mathrm{RQS}_{\\theta(x_{<i})}(x_i)$\n",
    "- Coupling Neural Spline Flow (NSF-CL); $f(x^{(2)}) = \\mathrm{RQS}_{\\theta(x^{(1)})}(x^{(2)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example\n",
    "![](http://akosiorek.github.io/resources/simple_flows.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons # here you can use any dataset you want\n",
    "\n",
    "X, _ = make_moons(n_samples=10000)\n",
    "X = X.astype('float32')\n",
    "dim = X.shape[1] # n_features\n",
    "dataset = TensorDataset(torch.from_numpy(X))\n",
    "dataloader = DataLoader(dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: old comet version (2.0.18) detected. current: 3.0.0 please update your comet lib with command: `pip install --no-cache-dir --upgrade comet_ml`\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/holybayes/yandex-school-nf/b8daefdc2e2044ab8966f1c4c8c6e4ea\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flows = [MAF(dim=dim, hidden_dim=10), MAF(dim=dim, hidden_dim=10), \n",
    "         MAF(dim=dim, hidden_dim=10), MAF(dim=dim, hidden_dim=10)] # define list of flows here (you can stack multiple ones)\n",
    "prior = MultivariateNormal(torch.zeros(dim), torch.eye(dim)) # define prior (domain) distribution (multinormal with zero bias and identity covariance matrix)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "model = NormalizingFlowModel(dim, prior, flows).to(device) # define model\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005) # define the optimizer to fit the parameters of flow\n",
    "\n",
    "experiment = Experiment(api_key=\"lODeHEtCf7XLaV6DJrOfugNcA\",\n",
    "                        project_name=\"yandex-school-nf\", workspace=\"holybayes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "plt.scatter(X[:,0], X[:,1])\n",
    "plt.xlim(-1,2)\n",
    "plt.ylim(-1,2)\n",
    "\n",
    "experiment.log_figure('original data', overwrite=True, figure=plt)\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoches = 1000\n",
    "for epoch in range(epoches):\n",
    "    model.train()\n",
    "    for step, batch_x in enumerate(dataloader):\n",
    "        batch_x = batch_x[0].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        z, logp_z, log_det = model(batch_x)\n",
    "        logp_x = logp_z + log_det # \"+\" because in this implementation f is inverse\n",
    "        loss = -torch.mean(logp_x) # train the flow by maximizing the likelihood of final distribution\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        experiment.log_metric('Loss', loss.mean().detach(), step, epoch)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        # Test\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_test = np.random.uniform(-1,2, (10000,2)).astype('float32')\n",
    "            z, logp_z, log_det = model(torch.from_numpy(X_test))\n",
    "            logp_x = logp_z + log_det\n",
    "            plt.figure(figsize = (10,10))\n",
    "            plt.scatter(X_test[:,0], X_test[:,1], c=logp_x)\n",
    "\n",
    "            experiment.log_figure('Predicted density', step=epoch)\n",
    "            plt.clf()\n",
    "            \n",
    "            X_sampled = model.sample(10000)\n",
    "            plt.figure(figsize = (10,10))\n",
    "            plt.scatter(X_sampled[:,0], X_sampled[:,1], c=logp_x)\n",
    "\n",
    "            experiment.log_figure('Samples', step=epoch)\n",
    "            plt.clf()\n",
    "experiment.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem\n",
    "Show the most likely and unlikely samples. Is NF good for anomaly detection? Implement simple anomalies classifier based on NF and evaluate its quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
