{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/HSE-LAMBDA/DeepGenerativeModels/blob/master/seminars/seminar-9/1.aae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.CenterCrop(160),\n",
    "        transforms.Scale(64),\n",
    "        transforms.ToTensor(),\n",
    "         #transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "dataset = datasets.CIFAR10(transform)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=20, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True\n",
    "cnt = 0\n",
    "lr = 1e-4\n",
    "out_dir = \"out_aae3\"\n",
    "batch_size = 256\n",
    "\n",
    "nc = 3 # number of channels\n",
    "nz = 64 # size of latent vector\n",
    "ngf = 64 # decoder (generator) filter factor\n",
    "ndf = 64 # encoder filter factor\n",
    "h_dim = 128 # discriminator hidden size\n",
    "lam = 1 # regulization coefficient\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.main = [\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(     nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(    ngf,      nc, 4, 2, 1, bias=False),\n",
    "            nn.Sigmoid()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        ]\n",
    "        for idx, module in enumerate(self.main):\n",
    "            self.add_module(str(idx), module)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.main:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.main = [\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, nz, 4, 1, 0, bias=False),\n",
    "        ]\n",
    "\n",
    "        for idx, module in enumerate(self.main):\n",
    "            self.add_module(str(idx), module)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.main:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = [\n",
    "            nn.Linear(nz, h_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        ]\n",
    "        for idx, module in enumerate(self.main):\n",
    "            self.add_module(str(idx), module)\n",
    "    def forward(self, x):\n",
    "        for layer in self.main:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "Q = Encoder().to(device)\n",
    "P = Decoder().to(device)\n",
    "D = Discriminator().to(device)\n",
    "\n",
    "def reset_grad():\n",
    "    Q.zero_grad()\n",
    "    P.zero_grad()\n",
    "    D.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_solver = optim.Adam(Q.parameters(), lr=lr)\n",
    "P_solver = optim.Adam(P.parameters(), lr=lr)\n",
    "D_solver = optim.Adam(D.parameters(), lr=lr*0.1)\n",
    "\n",
    "for it in range(100000):\n",
    "\n",
    "    for batch_idx, batch_item in enumerate(data_loader):\n",
    "        #X = sample_X(mb_size)\n",
    "        \"\"\" Reconstruction phase \"\"\"\n",
    "        X = Variable(batch_item[0])\n",
    "        if cuda:\n",
    "            X = X.cuda()\n",
    "\n",
    "        z_sample = Q(X)\n",
    "\n",
    "        X_sample = P(z_sample)\n",
    "        recon_loss = F.mse_loss(X_sample, X)\n",
    "\n",
    "        recon_loss.backward()\n",
    "        P_solver.step()\n",
    "        Q_solver.step()\n",
    "        reset_grad()\n",
    "\n",
    "        \"\"\" Regularization phase \"\"\"\n",
    "        # Discriminator\n",
    "        for _ in range(5):\n",
    "            z_real = Variable(torch.randn(batch_size, nz))\n",
    "            if cuda:\n",
    "                z_real = z_real.cuda()\n",
    "\n",
    "            z_fake = Q(X).view(batch_size,-1)\n",
    "\n",
    "            D_real = D(z_real)\n",
    "            D_fake = D(z_fake)\n",
    "\n",
    "            #D_loss = -torch.mean(torch.log(D_real) + torch.log(1 - D_fake))\n",
    "            D_loss = -(torch.mean(D_real) - torch.mean(D_fake))\n",
    "\n",
    "            D_loss.backward()\n",
    "            D_solver.step()\n",
    "\n",
    "            # Weight clipping\n",
    "            for p in D.parameters():\n",
    "                p.data.clamp_(-0.01, 0.01)\n",
    "\n",
    "            reset_grad()\n",
    "\n",
    "        # Generator\n",
    "        z_fake = Q(X).view(batch_size,-1)\n",
    "        D_fake = D(z_fake)\n",
    "\n",
    "        #G_loss = -torch.mean(torch.log(D_fake))\n",
    "        G_loss = -torch.mean(D_fake)\n",
    "\n",
    "        G_loss.backward()\n",
    "        Q_solver.step()\n",
    "        reset_grad()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Iter-{}; D_loss: {:.4}; G_loss: {:.4}; recon_loss: {:.4}'\n",
    "                  .format(batch_idx, D_loss.data[0], G_loss.data[0], recon_loss.data[0]))\n",
    "            torch.save(Q,\"Q_latest.pth\")\n",
    "            torch.save(P,\"P_latest.pth\")\n",
    "            torch.save(D,\"D_latest.pth\")\n",
    "\n",
    "        # Print and plot every now and then\n",
    "        if batch_idx % 100 == 0:\n",
    "\n",
    "            z_real = z_real.unsqueeze(2).unsqueeze(3) # add 2 dimensions\n",
    "            if cnt % 2 == 0:\n",
    "                samples = P(z_real) # Generated\n",
    "            else:\n",
    "                samples = X_sample # Reconstruction\n",
    "            #samples = X_sample\n",
    "            if cuda:\n",
    "                samples = samples.cpu()\n",
    "            samples = samples.data.numpy()[:16]\n",
    "\n",
    "            fig = plt.figure(figsize=(4, 4))\n",
    "            gs = gridspec.GridSpec(4, 4)\n",
    "            gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "            for i, sample in enumerate(samples):\n",
    "                ax = plt.subplot(gs[i])\n",
    "                plt.axis('off')\n",
    "                ax.set_xticklabels([])\n",
    "                ax.set_yticklabels([])\n",
    "                ax.set_aspect('equal')\n",
    "                sample = np.swapaxes(sample,0,2)\n",
    "                plt.imshow(sample)\n",
    "\n",
    "            \n",
    "            \n",
    "            cnt += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
